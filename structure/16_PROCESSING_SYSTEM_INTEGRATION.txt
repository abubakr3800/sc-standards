================================================================================
PROCESSING SYSTEM INTEGRATION - Complete Processing Pipeline
================================================================================

PURPOSE: Integration of enhanced extraction with AI training system
LOCATION: src/ai_standards/processing/
FILES: 4 processing modules with detailed code illustrations

================================================================================
PROCESSING FILES OVERVIEW
================================================================================

1. enhanced_dialux_extractor.py - Advanced PDF extraction with OCR
2. improve_standards_extraction.py - Standards data improvement
3. process_pdfs.py - PDF processing pipeline
4. process_standards.py - Standards processing workflow

================================================================================
ENHANCED DIALUX EXTRACTOR INTEGRATION
================================================================================

FILE: src/ai_standards/processing/enhanced_dialux_extractor.py
PURPOSE: Multi-method PDF extraction with OCR and advanced table processing

INTEGRATION WITH AI TRAINING:
```python
# Integration with AI training system
def integrate_with_training(self, pdf_path: Path) -> Dict[str, Any]:
    """Extract data and prepare for AI training"""
    # Extract comprehensive data
    extraction_result = self.extract_report(pdf_path, "training_data/")
    
    # Prepare for AI training
    training_data = {
        "text_chunks": [],
        "embeddings": [],
        "metadata": []
    }
    
    # Process text pages
    for page in extraction_result["text_pages"]:
        text = page["text"]
        chunks = self._split_into_chunks(text, chunk_size=1000, overlap=200)
        
        for chunk in chunks:
            training_data["text_chunks"].append({
                "text": chunk,
                "page": page["page"],
                "source": "pdfplumber",
                "confidence": 0.9
            })
    
    # Process OCR text
    for ocr_text in extraction_result["ocr_text"]:
        text = ocr_text["text"]
        chunks = self._split_into_chunks(text, chunk_size=1000, overlap=200)
        
        for chunk in chunks:
            training_data["text_chunks"].append({
                "text": chunk,
                "page": ocr_text["page"],
                "source": "ocr",
                "confidence": 0.8
            })
    
    # Process luminaire data
    for luminaire in extraction_result["luminaires"]:
        luminaire_text = self._format_luminaire_text(luminaire)
        training_data["text_chunks"].append({
            "text": luminaire_text,
            "page": 0,
            "source": "luminaire_table",
            "confidence": 0.95
        })
    
    return training_data

def _split_into_chunks(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """Split text into overlapping chunks for training"""
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - overlap
    
    return chunks

def _format_luminaire_text(self, luminaire: Dict) -> str:
    """Format luminaire data as text for training"""
    text_parts = []
    
    for key, value in luminaire.items():
        if value and str(value).strip():
            text_parts.append(f"{key}: {value}")
    
    return " | ".join(text_parts)
```

================================================================================
IMPROVE STANDARDS EXTRACTION INTEGRATION
================================================================================

FILE: src/ai_standards/processing/improve_standards_extraction.py
PURPOSE: Enhanced standards data extraction and processing

ENHANCED PATTERN RECOGNITION:
```python
class ImprovedStandardsExtractor:
    """Enhanced standards extraction with advanced pattern recognition"""
    
    def __init__(self):
        # Advanced pattern definitions
        self.illuminance_patterns = [
            r'(\d+)\s*lux\s*(?:minimum|min|≥|>=)',
            r'(?:minimum|min|≥|>=)\s*(\d+)\s*lux',
            r'illuminance[:\s]*(\d+)\s*lux',
            r'(\d+)\s*lux\s*(?:to|-)\s*(\d+)\s*lux',
            r'(\d+)\s*-\s*(\d+)\s*lux',
            r'office[:\s]*(\d+)\s*lux',
            r'conference[:\s]*(\d+)\s*lux',
            r'corridor[:\s]*(\d+)\s*lux'
        ]
        
        self.ugr_patterns = [
            r'UGR[:\s]*(\d+(?:\.\d+)?)',
            r'unified\s+glare\s+rating[:\s]*(\d+(?:\.\d+)?)',
            r'glare[:\s]*(\d+(?:\.\d+)?)',
            r'UGR\s*≤\s*(\d+(?:\.\d+)?)',
            r'UGR\s*<=\s*(\d+(?:\.\d+)?)'
        ]
        
        self.cri_patterns = [
            r'CRI[:\s]*(\d+)',
            r'color\s+rendering\s+index[:\s]*(\d+)',
            r'Ra[:\s]*(\d+)',
            r'CRI\s*≥\s*(\d+)',
            r'CRI\s*>=\s*(\d+)'
        ]
    
    def extract_structured_tables(self, text: str) -> List[Dict[str, Any]]:
        """Extract structured data from table-like text"""
        tables = []
        
        # Split text into lines
        lines = text.split('\n')
        
        # Find table-like structures
        current_table = []
        in_table = False
        
        for line in lines:
            line = line.strip()
            
            # Check if line looks like table header
            if self._is_table_header(line):
                if current_table:
                    tables.append(self._process_table(current_table))
                current_table = [line]
                in_table = True
            
            # Check if line looks like table row
            elif in_table and self._is_table_row(line):
                current_table.append(line)
            
            # End of table
            elif in_table and not self._is_table_row(line):
                if current_table:
                    tables.append(self._process_table(current_table))
                current_table = []
                in_table = False
        
        # Process final table
        if current_table:
            tables.append(self._process_table(current_table))
        
        return tables
    
    def _is_table_header(self, line: str) -> bool:
        """Check if line is a table header"""
        header_indicators = [
            'room', 'area', 'illuminance', 'ugr', 'cri', 'uniformity',
            'power', 'density', 'application', 'type', 'level'
        ]
        
        line_lower = line.lower()
        return any(indicator in line_lower for indicator in header_indicators)
    
    def _is_table_row(self, line: str) -> bool:
        """Check if line is a table row"""
        # Look for multiple values separated by spaces or tabs
        parts = line.split()
        return len(parts) >= 3 and any(part.replace('.', '').replace(',', '').isdigit() for part in parts)
    
    def _process_table(self, table_lines: List[str]) -> Dict[str, Any]:
        """Process table lines into structured data"""
        if not table_lines:
            return {}
        
        # Parse header
        header = table_lines[0].split()
        
        # Parse rows
        rows = []
        for line in table_lines[1:]:
            row_data = line.split()
            if len(row_data) >= len(header):
                row_dict = {}
                for i, value in enumerate(row_data[:len(header)]):
                    row_dict[header[i]] = value
                rows.append(row_dict)
        
        return {
            "header": header,
            "rows": rows,
            "row_count": len(rows)
        }
    
    def extract_lighting_parameters(self, text: str) -> Dict[str, Any]:
        """Extract lighting parameters using advanced patterns"""
        parameters = {
            "illuminance": [],
            "ugr": [],
            "cri": [],
            "uniformity": [],
            "power_density": []
        }
        
        # Extract illuminance values
        for pattern in self.illuminance_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    # Range values
                    try:
                        min_val = float(match[0])
                        max_val = float(match[1]) if len(match) > 1 else min_val
                        parameters["illuminance"].append({
                            "min": min_val,
                            "max": max_val,
                            "unit": "lux",
                            "pattern": pattern
                        })
                    except ValueError:
                        continue
                else:
                    # Single values
                    try:
                        value = float(match)
                        parameters["illuminance"].append({
                            "value": value,
                            "unit": "lux",
                            "pattern": pattern
                        })
                    except ValueError:
                        continue
        
        # Extract UGR values
        for pattern in self.ugr_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                try:
                    value = float(match)
                    parameters["ugr"].append({
                        "value": value,
                        "unit": "UGR",
                        "pattern": pattern
                    })
                except ValueError:
                    continue
        
        # Extract CRI values
        for pattern in self.cri_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                try:
                    value = int(match)
                    parameters["cri"].append({
                        "value": value,
                        "unit": "CRI",
                        "pattern": pattern
                    })
                except ValueError:
                    continue
        
        return parameters
```

================================================================================
PROCESS PDFS INTEGRATION
================================================================================

FILE: src/ai_standards/processing/process_pdfs.py
PURPOSE: PDF processing pipeline with enhanced extraction

COMPLETE PROCESSING PIPELINE:
```python
class PDFProcessingPipeline:
    """Complete PDF processing pipeline with enhanced extraction"""
    
    def __init__(self):
        self.enhanced_extractor = EnhancedDialuxExtractor()
        self.standards_extractor = ImprovedStandardsExtractor()
        self.ai_trainer = AIStandardsTrainer()
    
    def process_pdf_pipeline(self, pdf_path: Path, output_dir: str = "processed") -> Dict[str, Any]:
        """Complete PDF processing pipeline"""
        logger.info(f"Starting PDF processing pipeline for: {pdf_path}")
        
        # Step 1: Enhanced extraction
        logger.info("Step 1: Enhanced PDF extraction...")
        extraction_result = self.enhanced_extractor.extract_report(pdf_path, output_dir)
        
        # Step 2: Standards data extraction
        logger.info("Step 2: Standards data extraction...")
        all_text = self._combine_all_text(extraction_result)
        standards_data = self.standards_extractor.extract_lighting_parameters(all_text)
        structured_tables = self.standards_extractor.extract_structured_tables(all_text)
        
        # Step 3: Prepare training data
        logger.info("Step 3: Preparing training data...")
        training_data = self._prepare_training_data(extraction_result, standards_data, structured_tables)
        
        # Step 4: Generate embeddings
        logger.info("Step 4: Generating embeddings...")
        embeddings = self.ai_trainer.generate_embeddings(training_data["texts"])
        
        # Step 5: Store in database
        logger.info("Step 5: Storing in database...")
        self.ai_trainer.store_embeddings_in_db(embeddings, training_data["texts"], training_data["metadata"])
        
        # Step 6: Create comprehensive result
        result = {
            "extraction_result": extraction_result,
            "standards_data": standards_data,
            "structured_tables": structured_tables,
            "training_data": training_data,
            "embeddings_generated": len(embeddings),
            "database_updated": True,
            "processing_time": datetime.now().isoformat()
        }
        
        # Save comprehensive result
        result_path = Path(output_dir) / f"{pdf_path.stem}_complete_processing.json"
        with open(result_path, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        logger.info(f"PDF processing pipeline completed. Results saved to: {result_path}")
        return result
    
    def _combine_all_text(self, extraction_result: Dict[str, Any]) -> str:
        """Combine all text sources from extraction result"""
        all_text = ""
        
        # Add PDFPlumber text
        for page in extraction_result["text_pages"]:
            all_text += page["text"] + "\n"
        
        # Add OCR text
        for ocr_text in extraction_result["ocr_text"]:
            all_text += ocr_text["text"] + "\n"
        
        return all_text
    
    def _prepare_training_data(self, extraction_result: Dict[str, Any], 
                             standards_data: Dict[str, Any], 
                             structured_tables: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Prepare comprehensive training data"""
        training_data = {
            "texts": [],
            "labels": [],
            "metadata": []
        }
        
        # Add text chunks from extraction
        for page in extraction_result["text_pages"]:
            text = page["text"]
            chunks = self._split_into_chunks(text, chunk_size=1000, overlap=200)
            
            for chunk in chunks:
                training_data["texts"].append(chunk)
                training_data["labels"].append(self._classify_text(chunk))
                training_data["metadata"].append({
                    "source": "pdfplumber",
                    "page": page["page"],
                    "confidence": 0.9
                })
        
        # Add OCR text chunks
        for ocr_text in extraction_result["ocr_text"]:
            text = ocr_text["text"]
            chunks = self._split_into_chunks(text, chunk_size=1000, overlap=200)
            
            for chunk in chunks:
                training_data["texts"].append(chunk)
                training_data["labels"].append(self._classify_text(chunk))
                training_data["metadata"].append({
                    "source": "ocr",
                    "page": ocr_text["page"],
                    "confidence": 0.8
                })
        
        # Add structured data as text
        for table in structured_tables:
            table_text = self._format_table_as_text(table)
            training_data["texts"].append(table_text)
            training_data["labels"].append(6)  # Table category
            training_data["metadata"].append({
                "source": "structured_table",
                "page": 0,
                "confidence": 0.95
            })
        
        return training_data
    
    def _split_into_chunks(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """Split text into overlapping chunks"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            start = end - overlap
        
        return chunks
    
    def _classify_text(self, text: str) -> int:
        """Classify text into categories"""
        text_lower = text.lower()
        
        categories = {
            0: ["illuminance", "lux", "lighting level"],
            1: ["ugr", "glare", "unified glare rating"],
            2: ["cri", "color rendering", "color quality"],
            3: ["uniformity", "uniform lighting"],
            4: ["power density", "energy consumption"],
            5: ["safety", "emergency lighting"],
            6: ["table", "schedule", "list"],
            7: ["installation", "mounting"],
            8: ["maintenance", "cleaning"],
            9: ["general", "overview"]
        }
        
        scores = {}
        for category, keywords in categories.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            scores[category] = score
        
        return max(scores, key=scores.get) if max(scores.values()) > 0 else 9
    
    def _format_table_as_text(self, table: Dict[str, Any]) -> str:
        """Format table data as text"""
        if not table:
            return ""
        
        text_parts = []
        
        # Add header
        if "header" in table:
            text_parts.append(" | ".join(table["header"]))
        
        # Add rows
        if "rows" in table:
            for row in table["rows"]:
                row_text = " | ".join([str(value) for value in row.values()])
                text_parts.append(row_text)
        
        return "\n".join(text_parts)
```

================================================================================
PROCESS STANDARDS INTEGRATION
================================================================================

FILE: src/ai_standards/processing/process_standards.py
PURPOSE: Standards processing workflow with AI training integration

STANDARDS PROCESSING WORKFLOW:
```python
class StandardsProcessingWorkflow:
    """Standards processing workflow with AI training integration"""
    
    def __init__(self):
        self.enhanced_extractor = EnhancedDialuxExtractor()
        self.standards_extractor = ImprovedStandardsExtractor()
        self.ai_trainer = AIStandardsTrainer()
    
    def process_standards_workflow(self, standards_dir: Path, output_dir: str = "processed") -> Dict[str, Any]:
        """Complete standards processing workflow"""
        logger.info(f"Starting standards processing workflow for: {standards_dir}")
        
        # Find all PDF files
        pdf_files = list(standards_dir.glob("*.pdf"))
        logger.info(f"Found {len(pdf_files)} PDF files to process")
        
        # Process each PDF
        all_results = []
        all_training_data = {
            "texts": [],
            "labels": [],
            "metadata": []
        }
        
        for pdf_file in pdf_files:
            logger.info(f"Processing: {pdf_file.name}")
            
            # Process individual PDF
            result = self._process_single_standard(pdf_file, output_dir)
            all_results.append(result)
            
            # Collect training data
            if "training_data" in result:
                all_training_data["texts"].extend(result["training_data"]["texts"])
                all_training_data["labels"].extend(result["training_data"]["labels"])
                all_training_data["metadata"].extend(result["training_data"]["metadata"])
        
        # Train AI model with all data
        logger.info("Training AI model with all standards data...")
        training_results = self.ai_trainer.train_from_processed_data([{
            "chunks": [{"text": text, "id": i, "page": 0, "confidence": 0.9} 
                      for i, text in enumerate(all_training_data["texts"])]
        }])
        
        # Create comprehensive workflow result
        workflow_result = {
            "processed_files": len(pdf_files),
            "individual_results": all_results,
            "training_data_summary": {
                "total_texts": len(all_training_data["texts"]),
                "total_labels": len(set(all_training_data["labels"])),
                "categories": len(set(all_training_data["labels"]))
            },
            "training_results": training_results,
            "workflow_completed": True,
            "completion_time": datetime.now().isoformat()
        }
        
        # Save workflow result
        result_path = Path(output_dir) / "standards_workflow_result.json"
        with open(result_path, "w", encoding="utf-8") as f:
            json.dump(workflow_result, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Standards processing workflow completed. Results saved to: {result_path}")
        return workflow_result
    
    def _process_single_standard(self, pdf_file: Path, output_dir: str) -> Dict[str, Any]:
        """Process a single standards PDF"""
        # Use the PDF processing pipeline
        pipeline = PDFProcessingPipeline()
        result = pipeline.process_pdf_pipeline(pdf_file, output_dir)
        
        return result
```

================================================================================
INTEGRATION WITH MAIN SYSTEM
================================================================================

INTEGRATION WITH MAIN.PY:
```python
# In main.py - Integration with processing system
def process_standards_with_enhanced_extraction(standards_dir: Path, output_dir: str = "processed"):
    """Process standards with enhanced extraction and AI training"""
    from ai_standards.processing.process_standards import StandardsProcessingWorkflow
    
    workflow = StandardsProcessingWorkflow()
    result = workflow.process_standards_workflow(standards_dir, output_dir)
    
    return result

def train_with_enhanced_data(processed_data_dir: Path):
    """Train AI model with enhanced processed data"""
    from ai_standards.processing.process_pdfs import PDFProcessingPipeline
    from ai_standards.models.ai_trainer import AIStandardsTrainer
    
    # Load processed data
    processed_files = list(processed_data_dir.glob("*_complete_processing.json"))
    
    # Prepare training data
    all_training_data = []
    for file in processed_files:
        with open(file, "r", encoding="utf-8") as f:
            data = json.load(f)
            if "training_data" in data:
                all_training_data.extend(data["training_data"]["texts"])
    
    # Train AI model
    trainer = AIStandardsTrainer()
    results = trainer.train_from_processed_data([{
        "chunks": [{"text": text, "id": i, "page": 0, "confidence": 0.9} 
                  for i, text in enumerate(all_training_data)]
    }])
    
    return results
```

================================================================================
PERFORMANCE OPTIMIZATION
================================================================================

OPTIMIZATION FEATURES:
- Parallel processing of multiple PDFs
- Batch processing for efficiency
- Memory management for large files
- Caching of intermediate results
- Progress tracking and logging

MEMORY MANAGEMENT:
- Processes files individually
- Clears intermediate results
- Uses efficient data structures
- Implements garbage collection

================================================================================
ERROR HANDLING
================================================================================

ERROR TYPES HANDLED:
1. PDF processing errors
2. OCR failures
3. Table extraction errors
4. AI training failures
5. Database connection issues
6. File I/O errors

ERROR HANDLING STRATEGY:
- Graceful fallback mechanisms
- Detailed error logging
- Partial result recovery
- User-friendly error messages
- Robust error recovery

================================================================================
USAGE EXAMPLES
================================================================================

1. PROCESS SINGLE PDF:
   ```python
   from ai_standards.processing.process_pdfs import PDFProcessingPipeline
   
   pipeline = PDFProcessingPipeline()
   result = pipeline.process_pdf_pipeline("standard.pdf", "output/")
   ```

2. PROCESS STANDARDS DIRECTORY:
   ```python
   from ai_standards.processing.process_standards import StandardsProcessingWorkflow
   
   workflow = StandardsProcessingWorkflow()
   result = workflow.process_standards_workflow(Path("standards/"), "output/")
   ```

3. ENHANCED EXTRACTION ONLY:
   ```python
   from ai_standards.processing.enhanced_dialux_extractor import EnhancedDialuxExtractor
   
   extractor = EnhancedDialuxExtractor()
   result = extractor.extract_report("report.pdf", "output/")
   ```

4. COMMAND LINE USAGE:
   ```bash
   py main.py process --input-dir standards/ --output-dir processed/
   py main.py train --data-dir processed/
   ```

================================================================================
DEPENDENCIES
================================================================================

REQUIRED PACKAGES:
- All packages from requirements.txt
- Enhanced extraction dependencies (pytesseract, Pillow, camelot-py)

INTEGRATION DEPENDENCIES:
- AI training system
- Vector database
- Configuration system
- Logging system

================================================================================

