================================================================================
AI TRAINING SYSTEM DETAILED - src/ai_standards/models/ai_trainer.py
================================================================================

FILE: src/ai_standards/models/ai_trainer.py
PURPOSE: AI model training, fine-tuning, and embedding generation with detailed code illustrations
LOCATION: src/ai_standards/models/
DEPENDENCIES:
- logging: Standard logging
- pathlib.Path: Path handling
- typing: Type hints (Dict, List, Optional, Tuple, Any)
- json: JSON handling
- pickle: Object serialization
- datetime: Date/time handling
- torch: PyTorch deep learning framework
- numpy: Numerical computing
- pandas: Data manipulation
- transformers: Hugging Face transformers
- sentence_transformers: Sentence embedding models
- sklearn: Machine learning utilities
- chromadb: Vector database
- loguru: Advanced logging

================================================================================
CLASS STRUCTURE
================================================================================

CLASS: AIStandardsTrainer
PURPOSE: Handles AI model training for standards understanding
INITIALIZATION:
- device: PyTorch device (CUDA if available, else CPU)
- embedding_model: Sentence transformer model
- classification_model: Classification model
- tokenizer: Text tokenizer
- vector_db: Vector database client
- pdf_processor: PDF processing instance

================================================================================
INITIALIZATION METHODS WITH CODE ILLUSTRATIONS
================================================================================

1. __init__()
   PURPOSE: Initialize the AI trainer with models and database
   PARAMETERS: None
   RETURNS: None
   FUNCTIONALITY: Sets up device, models, and vector database
   
   CODE ILLUSTRATION:
   ```python
   def __init__(self):
       self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
       self.embedding_model = None
       self.classification_model = None
       self.tokenizer = None
       self.vector_db = None
       self.pdf_processor = PDFProcessor()
       self._initialize_models()
       self._initialize_vector_db()
   ```

2. _initialize_models()
   PURPOSE: Initialize AI models with configuration
   PARAMETERS: None
   RETURNS: None
   FUNCTIONALITY: Loads embedding and classification models
   
   CODE ILLUSTRATION:
   ```python
   def _initialize_models(self):
       """Initialize AI models"""
       logger.info("Initializing AI models...")
       
       # Initialize embedding model
       model_name = config.AI_MODELS["embedding_model"]
       self.embedding_model = SentenceTransformer(model_name)
       logger.info(f"Loaded embedding model: {model_name}")
       
       # Initialize classification model and tokenizer
       classification_model_name = config.AI_MODELS["classification_model"]
       self.tokenizer = AutoTokenizer.from_pretrained(classification_model_name)
       self.classification_model = AutoModelForSequenceClassification.from_pretrained(
           classification_model_name,
           num_labels=10  # Number of standard categories
       )
       logger.info(f"Loaded classification model: {classification_model_name}")
   ```

3. _initialize_vector_db()
   PURPOSE: Initialize vector database for embeddings
   PARAMETERS: None
   RETURNS: None
   FUNCTIONALITY: Sets up ChromaDB persistent client and collection
   
   CODE ILLUSTRATION:
   ```python
   def _initialize_vector_db(self):
       """Initialize vector database for embeddings"""
       try:
           self.vector_db = chromadb.PersistentClient(
               path=str(config.DATABASE["vector_db_path"])
           )
           self.collection = self.vector_db.get_or_create_collection(
               name="standards_embeddings",
               metadata={"description": "Standards document embeddings"}
           )
           logger.info("Initialized vector database")
       except Exception as e:
           logger.error(f"Failed to initialize vector database: {e}")
           self.vector_db = None
   ```

================================================================================
TRAINING DATA CREATION WITH CODE ILLUSTRATIONS
================================================================================

4. create_training_data(processed_documents: List[Dict[str, Any]]) -> Dict[str, List]
   PURPOSE: Create training data from processed documents
   PARAMETERS:
   - processed_documents: List of processed PDF documents
   RETURNS: Dictionary with training data
   FUNCTIONALITY: Processes document chunks and creates embeddings
   
   CODE ILLUSTRATION:
   ```python
   def create_training_data(self, processed_documents: List[Dict[str, Any]]) -> Dict[str, List]:
       """
       Create training data from processed documents
       
       Args:
           processed_documents: List of processed PDF documents
           
       Returns:
           Dictionary with training data
       """
       logger.info("Creating training data...")
       
       training_data = {
           "texts": [],
           "labels": [],
           "embeddings": [],
           "metadata": []
       }
       
       for doc in processed_documents:
           # Extract chunks and create embeddings
           for chunk in doc["chunks"]:
               text = chunk["text"]
               
               # Create embedding
               embedding = self.embedding_model.encode(text)
               
               # Generate label based on content
               label = self._generate_label(text, doc)
               
               # Store training data
               training_data["texts"].append(text)
               training_data["labels"].append(label)
               training_data["embeddings"].append(embedding.tolist())
               training_data["metadata"].append({
                   "document_id": doc.get("id", ""),
                   "chunk_id": chunk.get("id", ""),
                   "page": chunk.get("page", 0),
                   "confidence": chunk.get("confidence", 0.0)
               })
       
       logger.info(f"Created training data: {len(training_data['texts'])} samples")
       return training_data
   ```

5. _generate_label(text: str, document: Dict) -> int
   PURPOSE: Generate label for text based on content analysis
   PARAMETERS:
   - text: Text content
   - document: Document metadata
   RETURNS: Integer label
   FUNCTIONALITY: Analyzes text content to determine category
   
   CODE ILLUSTRATION:
   ```python
   def _generate_label(self, text: str, document: Dict) -> int:
       """Generate label for text based on content analysis"""
       text_lower = text.lower()
       
       # Define category keywords
       categories = {
           0: ["illuminance", "lux", "lighting level", "brightness"],
           1: ["ugr", "glare", "unified glare rating", "glare control"],
           2: ["cri", "color rendering", "color quality", "color temperature"],
           3: ["uniformity", "uniform lighting", "light distribution"],
           4: ["power density", "energy consumption", "watt per square meter"],
           5: ["safety", "emergency lighting", "exit signs", "safety requirements"],
           6: ["measurement", "testing", "verification", "compliance"],
           7: ["installation", "mounting", "positioning", "layout"],
           8: ["maintenance", "cleaning", "replacement", "service"],
           9: ["general", "overview", "introduction", "summary"]
       }
       
       # Score each category
       scores = {}
       for category, keywords in categories.items():
           score = sum(1 for keyword in keywords if keyword in text_lower)
           scores[category] = score
       
       # Return category with highest score
       return max(scores, key=scores.get) if max(scores.values()) > 0 else 9
   ```

================================================================================
EMBEDDING GENERATION WITH CODE ILLUSTRATIONS
================================================================================

6. generate_embeddings(texts: List[str]) -> np.ndarray
   PURPOSE: Generate embeddings for text data
   PARAMETERS:
   - texts: List of text strings
   RETURNS: Numpy array of embeddings
   FUNCTIONALITY: Uses sentence transformer to create embeddings
   
   CODE ILLUSTRATION:
   ```python
   def generate_embeddings(self, texts: List[str]) -> np.ndarray:
       """Generate embeddings for text data"""
       logger.info(f"Generating embeddings for {len(texts)} texts...")
       
       # Process in batches for memory efficiency
       batch_size = config.AI_MODELS["batch_size"]
       embeddings = []
       
       for i in range(0, len(texts), batch_size):
           batch_texts = texts[i:i + batch_size]
           batch_embeddings = self.embedding_model.encode(
               batch_texts,
               convert_to_tensor=True,
               show_progress_bar=True
           )
           embeddings.append(batch_embeddings.cpu().numpy())
       
       # Concatenate all embeddings
       all_embeddings = np.vstack(embeddings)
       
       # Normalize embeddings
       all_embeddings = all_embeddings / np.linalg.norm(all_embeddings, axis=1, keepdims=True)
       
       logger.info(f"Generated embeddings shape: {all_embeddings.shape}")
       return all_embeddings
   ```

7. store_embeddings_in_db(embeddings: np.ndarray, texts: List[str], metadata: List[Dict])
   PURPOSE: Store embeddings in vector database
   PARAMETERS:
   - embeddings: Numpy array of embeddings
   - texts: List of corresponding texts
   - metadata: List of metadata dictionaries
   RETURNS: None
   FUNCTIONALITY: Stores embeddings in ChromaDB with metadata
   
   CODE ILLUSTRATION:
   ```python
   def store_embeddings_in_db(self, embeddings: np.ndarray, texts: List[str], metadata: List[Dict]):
       """Store embeddings in vector database"""
       if not self.vector_db:
           logger.warning("Vector database not available")
           return
       
       logger.info(f"Storing {len(embeddings)} embeddings in database...")
       
       # Convert embeddings to list format
       embeddings_list = embeddings.tolist()
       
       # Create unique IDs
       ids = [f"embedding_{i}_{hash(text[:50])}" for i, text in enumerate(texts)]
       
       # Prepare metadata
       db_metadata = []
       for i, meta in enumerate(metadata):
           db_meta = {
               "text_length": len(texts[i]),
               "document_id": meta.get("document_id", ""),
               "chunk_id": meta.get("chunk_id", ""),
               "page": meta.get("page", 0),
               "confidence": meta.get("confidence", 0.0)
           }
           db_metadata.append(db_meta)
       
       # Store in ChromaDB
       try:
           self.collection.add(
               embeddings=embeddings_list,
               documents=texts,
               metadatas=db_metadata,
               ids=ids
           )
           logger.info("Successfully stored embeddings in database")
       except Exception as e:
           logger.error(f"Failed to store embeddings: {e}")
   ```

================================================================================
MODEL TRAINING WITH CODE ILLUSTRATIONS
================================================================================

8. train_classification_model(training_data: Dict[str, List]) -> Dict[str, Any]
   PURPOSE: Train classification model
   PARAMETERS:
   - training_data: Training data dictionary
   RETURNS: Training results dictionary
   FUNCTIONALITY: Trains model with Hugging Face Trainer
   
   CODE ILLUSTRATION:
   ```python
   def train_classification_model(self, training_data: Dict[str, List]) -> Dict[str, Any]:
       """Train classification model"""
       logger.info("Starting classification model training...")
       
       # Split data
       X_train, X_val, y_train, y_val = train_test_split(
           training_data["texts"],
           training_data["labels"],
           test_size=0.2,
           random_state=42,
           stratify=training_data["labels"]
       )
       
       # Create datasets
       train_dataset = self._create_dataset(X_train, y_train)
       val_dataset = self._create_dataset(X_val, y_val)
       
       # Training arguments
       training_args = TrainingArguments(
           output_dir=str(config.MODELS_DIR / "classification"),
           num_train_epochs=config.AI_MODELS["num_epochs"],
           per_device_train_batch_size=config.AI_MODELS["batch_size"],
           per_device_eval_batch_size=config.AI_MODELS["batch_size"],
           warmup_steps=500,
           weight_decay=0.01,
           logging_dir=str(config.MODELS_DIR / "logs"),
           logging_steps=10,
           evaluation_strategy="steps",
           eval_steps=100,
           save_strategy="steps",
           save_steps=100,
           load_best_model_at_end=True,
           metric_for_best_model="accuracy",
           greater_is_better=True
       )
       
       # Data collator
       data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)
       
       # Trainer
       trainer = Trainer(
           model=self.classification_model,
           args=training_args,
           train_dataset=train_dataset,
           eval_dataset=val_dataset,
           data_collator=data_collator,
           compute_metrics=self._compute_metrics
       )
       
       # Train model
       trainer.train()
       
       # Evaluate model
       eval_results = trainer.evaluate()
       
       # Save model
       trainer.save_model()
       
       logger.info(f"Training completed. Evaluation results: {eval_results}")
       
       return {
           "training_loss": trainer.state.log_history[-1]["train_loss"],
           "eval_accuracy": eval_results["eval_accuracy"],
           "eval_loss": eval_results["eval_loss"],
           "model_path": str(config.MODELS_DIR / "classification")
       }
   ```

9. _create_dataset(texts: List[str], labels: List[int]) -> Dataset
   PURPOSE: Create PyTorch dataset for training
   PARAMETERS:
   - texts: List of text strings
   - labels: List of integer labels
   RETURNS: PyTorch Dataset
   FUNCTIONALITY: Tokenizes texts and creates dataset
   
   CODE ILLUSTRATION:
   ```python
   def _create_dataset(self, texts: List[str], labels: List[int]) -> Dataset:
       """Create PyTorch dataset for training"""
       # Tokenize texts
       encodings = self.tokenizer(
           texts,
           truncation=True,
           padding=True,
           max_length=config.AI_MODELS["max_sequence_length"],
           return_tensors="pt"
       )
       
       # Create dataset
       class StandardsDataset(Dataset):
           def __init__(self, encodings, labels):
               self.encodings = encodings
               self.labels = labels
           
           def __getitem__(self, idx):
               item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
               item['labels'] = torch.tensor(self.labels[idx])
               return item
           
           def __len__(self):
               return len(self.labels)
       
       return StandardsDataset(encodings, labels)
   ```

10. _compute_metrics(eval_pred) -> Dict[str, float]
    PURPOSE: Compute evaluation metrics
    PARAMETERS:
    - eval_pred: Evaluation predictions
    RETURNS: Dictionary with metrics
    FUNCTIONALITY: Calculates accuracy and other metrics
    
    CODE ILLUSTRATION:
    ```python
    def _compute_metrics(self, eval_pred) -> Dict[str, float]:
        """Compute evaluation metrics"""
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        
        accuracy = accuracy_score(labels, predictions)
        
        return {
            "accuracy": accuracy,
            "f1": f1_score(labels, predictions, average="weighted"),
            "precision": precision_score(labels, predictions, average="weighted"),
            "recall": recall_score(labels, predictions, average="weighted")
        }
    ```

================================================================================
FINE-TUNING WITH CODE ILLUSTRATIONS
================================================================================

11. fine_tune_embedding_model(training_data: Dict[str, List]) -> Dict[str, Any]
    PURPOSE: Fine-tune embedding model
    PARAMETERS:
    - training_data: Training data dictionary
    RETURNS: Fine-tuning results
    FUNCTIONALITY: Fine-tunes sentence transformer on standards data
    
    CODE ILLUSTRATION:
    ```python
    def fine_tune_embedding_model(self, training_data: Dict[str, List]) -> Dict[str, Any]:
        """Fine-tune embedding model"""
        logger.info("Starting embedding model fine-tuning...")
        
        # Create input examples
        train_examples = []
        for i, text in enumerate(training_data["texts"]):
            label = training_data["labels"][i]
            train_examples.append(InputExample(texts=[text], label=label))
        
        # Create data loader
        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=config.AI_MODELS["batch_size"])
        
        # Define loss function
        train_loss = losses.ContrastiveLoss(model=self.embedding_model)
        
        # Fine-tuning arguments
        args = {
            "output_path": str(config.MODELS_DIR / "embeddings"),
            "num_epochs": config.AI_MODELS["num_epochs"],
            "warmup_steps": 100,
            "optimizer_params": {"lr": config.AI_MODELS["learning_rate"]},
            "evaluation_steps": 100,
            "save_best_model": True
        }
        
        # Fine-tune model
        self.embedding_model.fit(
            train_objectives=[(train_dataloader, train_loss)],
            epochs=args["num_epochs"],
            warmup_steps=args["warmup_steps"],
            optimizer_params=args["optimizer_params"],
            evaluation_steps=args["evaluation_steps"],
            output_path=args["output_path"],
            save_best_model=args["save_best_model"]
        )
        
        logger.info("Embedding model fine-tuning completed")
        
        return {
            "model_path": args["output_path"],
            "epochs": args["num_epochs"],
            "learning_rate": config.AI_MODELS["learning_rate"]
        }
    ```

================================================================================
MODEL EVALUATION WITH CODE ILLUSTRATIONS
================================================================================

12. evaluate_model(model, test_data: Dict[str, List]) -> Dict[str, Any]
    PURPOSE: Evaluate trained model
    PARAMETERS:
    - model: Trained model
    - test_data: Test dataset
    RETURNS: Evaluation metrics
    FUNCTIONALITY: Evaluates model performance on test data
    
    CODE ILLUSTRATION:
    ```python
    def evaluate_model(self, model, test_data: Dict[str, List]) -> Dict[str, Any]:
        """Evaluate trained model"""
        logger.info("Evaluating model...")
        
        # Create test dataset
        test_dataset = self._create_dataset(test_data["texts"], test_data["labels"])
        
        # Create data loader
        test_dataloader = DataLoader(test_dataset, batch_size=config.AI_MODELS["batch_size"])
        
        # Evaluate model
        model.eval()
        predictions = []
        true_labels = []
        
        with torch.no_grad():
            for batch in test_dataloader:
                outputs = model(**batch)
                logits = outputs.logits
                preds = torch.argmax(logits, dim=-1)
                
                predictions.extend(preds.cpu().numpy())
                true_labels.extend(batch["labels"].cpu().numpy())
        
        # Calculate metrics
        accuracy = accuracy_score(true_labels, predictions)
        report = classification_report(true_labels, predictions, output_dict=True)
        
        logger.info(f"Model evaluation completed. Accuracy: {accuracy:.4f}")
        
        return {
            "accuracy": accuracy,
            "classification_report": report,
            "predictions": predictions,
            "true_labels": true_labels
        }
    ```

================================================================================
MODEL PERSISTENCE WITH CODE ILLUSTRATIONS
================================================================================

13. save_models(output_dir: Path)
    PURPOSE: Save trained models
    PARAMETERS:
    - output_dir: Directory to save models
    RETURNS: None
    FUNCTIONALITY: Saves all trained models and configurations
    
    CODE ILLUSTRATION:
    ```python
    def save_models(self, output_dir: Path):
        """Save trained models"""
        logger.info(f"Saving models to {output_dir}")
        
        # Create output directory
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Save embedding model
        if self.embedding_model:
            embedding_path = output_dir / "embeddings"
            self.embedding_model.save(str(embedding_path))
            logger.info(f"Saved embedding model to {embedding_path}")
        
        # Save classification model
        if self.classification_model:
            classification_path = output_dir / "classification"
            self.classification_model.save_pretrained(str(classification_path))
            self.tokenizer.save_pretrained(str(classification_path))
            logger.info(f"Saved classification model to {classification_path}")
        
        # Save training configuration
        config_data = {
            "embedding_model": config.AI_MODELS["embedding_model"],
            "classification_model": config.AI_MODELS["classification_model"],
            "max_sequence_length": config.AI_MODELS["max_sequence_length"],
            "batch_size": config.AI_MODELS["batch_size"],
            "learning_rate": config.AI_MODELS["learning_rate"],
            "num_epochs": config.AI_MODELS["num_epochs"],
            "saved_at": datetime.now().isoformat()
        }
        
        config_path = output_dir / "training_config.json"
        with open(config_path, "w") as f:
            json.dump(config_data, f, indent=2)
        
        logger.info(f"Saved training configuration to {config_path}")
    ```

================================================================================
COMPLETE TRAINING PIPELINE WITH CODE ILLUSTRATIONS
================================================================================

14. train_from_processed_data(processed_documents: List[Dict[str, Any]]) -> Dict[str, Any]
    PURPOSE: Complete training pipeline
    PARAMETERS:
    - processed_documents: List of processed documents
    RETURNS: Comprehensive training results
    FUNCTIONALITY: End-to-end training process
    
    CODE ILLUSTRATION:
    ```python
    def train_from_processed_data(self, processed_documents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Complete training pipeline"""
        logger.info("Starting complete training pipeline...")
        
        start_time = datetime.now()
        
        # 1. Create training data
        logger.info("Step 1: Creating training data...")
        training_data = self.create_training_data(processed_documents)
        
        # 2. Generate embeddings
        logger.info("Step 2: Generating embeddings...")
        embeddings = self.generate_embeddings(training_data["texts"])
        
        # 3. Store embeddings in database
        logger.info("Step 3: Storing embeddings in database...")
        self.store_embeddings_in_db(embeddings, training_data["texts"], training_data["metadata"])
        
        # 4. Train classification model
        logger.info("Step 4: Training classification model...")
        classification_results = self.train_classification_model(training_data)
        
        # 5. Fine-tune embedding model
        logger.info("Step 5: Fine-tuning embedding model...")
        embedding_results = self.fine_tune_embedding_model(training_data)
        
        # 6. Save models
        logger.info("Step 6: Saving models...")
        self.save_models(config.MODELS_DIR)
        
        # 7. Calculate training metrics
        end_time = datetime.now()
        training_time = (end_time - start_time).total_seconds()
        
        results = {
            "training_data": {
                "total_samples": len(training_data["texts"]),
                "total_embeddings": len(embeddings),
                "categories": len(set(training_data["labels"]))
            },
            "classification_results": classification_results,
            "embedding_results": embedding_results,
            "training_time_seconds": training_time,
            "models_saved": True,
            "database_updated": True
        }
        
        logger.info(f"Training pipeline completed in {training_time:.2f} seconds")
        logger.info(f"Results: {results}")
        
        return results
    ```

================================================================================
PERFORMANCE OPTIMIZATION
================================================================================

OPTIMIZATION FEATURES:
- GPU acceleration when available
- Batch processing for efficiency
- Memory management for large datasets
- Parallel processing where possible
- Model caching and persistence

MEMORY MANAGEMENT:
- Processes data in batches
- Clears GPU memory when needed
- Uses efficient data structures
- Implements garbage collection

================================================================================
ERROR HANDLING
================================================================================

ERROR TYPES HANDLED:
1. CUDA/GPU errors
2. Model loading failures
3. Training data errors
4. Database connection issues
5. Memory allocation errors
6. File I/O errors

ERROR HANDLING STRATEGY:
- Graceful fallback to CPU
- Detailed error logging
- Partial result recovery
- Model state preservation
- User-friendly error messages

================================================================================
USAGE EXAMPLES
================================================================================

1. BASIC TRAINING:
   ```python
   trainer = AIStandardsTrainer()
   results = trainer.train_from_processed_data(processed_docs)
   ```

2. STEP-BY-STEP TRAINING:
   ```python
   trainer = AIStandardsTrainer()
   
   # Create training data
   training_data = trainer.create_training_data(processed_docs)
   
   # Generate embeddings
   embeddings = trainer.generate_embeddings(training_data["texts"])
   
   # Store in database
   trainer.store_embeddings_in_db(embeddings, training_data["texts"], training_data["metadata"])
   
   # Train models
   classification_results = trainer.train_classification_model(training_data)
   embedding_results = trainer.fine_tune_embedding_model(training_data)
   
   # Save models
   trainer.save_models(Path("models/"))
   ```

3. MODEL EVALUATION:
   ```python
   # Load trained model
   model = AutoModelForSequenceClassification.from_pretrained("models/classification")
   
   # Evaluate
   results = trainer.evaluate_model(model, test_data)
   print(f"Accuracy: {results['accuracy']:.4f}")
   ```

================================================================================
DEPENDENCIES
================================================================================

REQUIRED PACKAGES:
- torch: PyTorch deep learning framework
- transformers: Hugging Face transformers
- sentence-transformers: Sentence embedding models
- sklearn: Machine learning utilities
- chromadb: Vector database
- numpy: Numerical computing
- pandas: Data manipulation

OPTIONAL PACKAGES:
- Standard library modules (logging, pathlib, typing, json, pickle, datetime)

================================================================================

